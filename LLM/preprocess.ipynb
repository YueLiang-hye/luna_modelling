{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68c8062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  studentID  meas  Iv1_state  Av1_state  Uv1_state  Co1_state  Co2_state  \\\n",
      "0  student1     8        3.0        3.0        4.0        4.0        4.0   \n",
      "1  student1    10        3.0        3.0        2.0        2.0        3.0   \n",
      "2  student1    12        3.0        2.0        3.0        2.0        2.0   \n",
      "3  student1    15        4.0        3.0        3.0        2.0        2.0   \n",
      "4  student1    17        3.0        2.0        3.0        3.0        4.0   \n",
      "\n",
      "   Angst_abbruch_state  Angst_scheitern_state  Leist_verstehen_state  ...  \\\n",
      "0                  2.0                    4.0                    3.0  ...   \n",
      "1                  1.0                    2.0                    4.0  ...   \n",
      "2                  1.0                    2.0                    4.0  ...   \n",
      "3                  4.0                    4.0                    3.0  ...   \n",
      "4                  1.0                    2.0                    3.0  ...   \n",
      "\n",
      "   PANN09_state  Block2_lerngruppe_state  Block2_abschreiben_state  \\\n",
      "0           4.0                      NaN                       NaN   \n",
      "1           1.0                      NaN                       NaN   \n",
      "2           4.0                      4.0                       4.0   \n",
      "3           4.0                      NaN                       NaN   \n",
      "4           3.0                      NaN                       NaN   \n",
      "\n",
      "   Block2_durchhaltevermoegen_state  Block2_vorNachbereitung_state  \\\n",
      "0                               NaN                            NaN   \n",
      "1                               NaN                            NaN   \n",
      "2                               2.0                            3.0   \n",
      "3                               NaN                            NaN   \n",
      "4                               NaN                            NaN   \n",
      "\n",
      "   Block2_zeitaufwand_state  Block2_anwesenheit_state  Block2_gruppe_state  \\\n",
      "0                       NaN                       NaN                  NaN   \n",
      "1                       NaN                       NaN                  NaN   \n",
      "2                       8.0                       3.0                  1.0   \n",
      "3                       NaN                       NaN                  NaN   \n",
      "4                       NaN                       NaN                  NaN   \n",
      "\n",
      "   Summe  event  \n",
      "0    NaN      0  \n",
      "1    NaN      0  \n",
      "2   89.0      0  \n",
      "3    NaN      0  \n",
      "4    NaN      0  \n",
      "\n",
      "[5 rows x 30 columns]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "## load data/TX/tx_long.csv\n",
    "data = pd.read_csv('data/TX/tx_long.csv')\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9f76422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns to drop (>50% missing): 8\n",
      "['Block2_lerngruppe_state', 'Block2_abschreiben_state', 'Block2_durchhaltevermoegen_state', 'Block2_vorNachbereitung_state', 'Block2_zeitaufwand_state', 'Block2_anwesenheit_state', 'Block2_gruppe_state', 'Summe']\n",
      "Shape before: (3187, 30)\n"
     ]
    }
   ],
   "source": [
    "# Drop columns with >50% missing values\n",
    "# Calculate missing percentage per column\n",
    "missing_pct = data.isnull().mean()\n",
    "\n",
    "# Columns to drop\n",
    "threshold = 0.5\n",
    "cols_to_drop = missing_pct[missing_pct > threshold].index.tolist()\n",
    "print(f\"Columns to drop (>{int(threshold*100)}% missing): {len(cols_to_drop)}\")\n",
    "print(cols_to_drop)\n",
    "\n",
    "# Show shape before\n",
    "print('Shape before:', data.shape)\n",
    "\n",
    "# Drop the columns\n",
    "data_clean = data.drop(columns=cols_to_drop)\n",
    "\n",
    "data_clean.to_csv('data/TX/tx_long_cleaned.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8faaeb8",
   "metadata": {},
   "source": [
    "## Preprocessing plan for CatBoost dropout model\n",
    "\n",
    "- Build target: predict next period dropout using current-row features. Create `next_event` = groupby(studentID)['event'].shift(-1) and drop rows where it's missing.\n",
    "- Strictly avoid leakage: all lag/rolling features use past data only (shifted by 1).\n",
    "- Create group-wise lags (lag 1,2,3) and rolling means for numeric features.\n",
    "- Add time-based features: days since previous measurement, days since first measurement, cumulative counts.\n",
    "- Handle missingness: forward-fill within student groups, then median (numerics) / mode or 'missing' (categoricals).\n",
    "- Leave categorical columns as string/category so CatBoost can accept them; pass `cat_features` when training.\n",
    "- Save preprocessed dataset to `data/TX/tx_long_preprocessed.csv` for model training.\n",
    "\n",
    "Run the preprocessing cell next to generate features and verify no leakage before training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b7c11ce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows before filtering at-risk: 3187, after at-risk filter: 3065\n",
      "Numeric features detected: ['Iv1_state', 'Av1_state', 'Uv1_state', 'Co1_state', 'Co2_state', 'Angst_abbruch_state', 'Angst_scheitern_state', 'Leist_verstehen_state', 'Leist_bearbeiten_state', 'Leist_stress_state', 'Leist_ueberfordert_state', 'Wiss_kommilitonen_state', 'Wiss_mathe_state', 'PANP01_state', 'PANP05_state', 'PANP08_state', 'PANN01_state', 'PANN05_state', 'PANN09_state']\n",
      "Categorical features detected: []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thinkpad\\AppData\\Local\\Temp\\ipykernel_3948\\2153889892.py:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'{c}_roll_mean_{w}'] = (\n",
      "C:\\Users\\thinkpad\\AppData\\Local\\Temp\\ipykernel_3948\\2153889892.py:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'{c}_roll_mean_{w}'] = (\n",
      "C:\\Users\\thinkpad\\AppData\\Local\\Temp\\ipykernel_3948\\2153889892.py:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'{c}_roll_mean_{w}'] = (\n",
      "C:\\Users\\thinkpad\\AppData\\Local\\Temp\\ipykernel_3948\\2153889892.py:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'{c}_roll_mean_{w}'] = (\n",
      "C:\\Users\\thinkpad\\AppData\\Local\\Temp\\ipykernel_3948\\2153889892.py:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'{c}_roll_mean_{w}'] = (\n",
      "C:\\Users\\thinkpad\\AppData\\Local\\Temp\\ipykernel_3948\\2153889892.py:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'{c}_roll_mean_{w}'] = (\n",
      "C:\\Users\\thinkpad\\AppData\\Local\\Temp\\ipykernel_3948\\2153889892.py:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'{c}_roll_mean_{w}'] = (\n",
      "C:\\Users\\thinkpad\\AppData\\Local\\Temp\\ipykernel_3948\\2153889892.py:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'{c}_roll_mean_{w}'] = (\n",
      "C:\\Users\\thinkpad\\AppData\\Local\\Temp\\ipykernel_3948\\2153889892.py:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'{c}_roll_mean_{w}'] = (\n",
      "C:\\Users\\thinkpad\\AppData\\Local\\Temp\\ipykernel_3948\\2153889892.py:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'{c}_roll_mean_{w}'] = (\n",
      "C:\\Users\\thinkpad\\AppData\\Local\\Temp\\ipykernel_3948\\2153889892.py:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'{c}_roll_mean_{w}'] = (\n",
      "C:\\Users\\thinkpad\\AppData\\Local\\Temp\\ipykernel_3948\\2153889892.py:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'{c}_roll_mean_{w}'] = (\n",
      "C:\\Users\\thinkpad\\AppData\\Local\\Temp\\ipykernel_3948\\2153889892.py:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'{c}_roll_mean_{w}'] = (\n",
      "C:\\Users\\thinkpad\\AppData\\Local\\Temp\\ipykernel_3948\\2153889892.py:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'{c}_roll_mean_{w}'] = (\n",
      "C:\\Users\\thinkpad\\AppData\\Local\\Temp\\ipykernel_3948\\2153889892.py:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'{c}_roll_mean_{w}'] = (\n",
      "C:\\Users\\thinkpad\\AppData\\Local\\Temp\\ipykernel_3948\\2153889892.py:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'{c}_roll_mean_{w}'] = (\n",
      "C:\\Users\\thinkpad\\AppData\\Local\\Temp\\ipykernel_3948\\2153889892.py:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'{c}_roll_mean_{w}'] = (\n",
      "C:\\Users\\thinkpad\\AppData\\Local\\Temp\\ipykernel_3948\\2153889892.py:60: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'{c}_roll_mean_{w}'] = (\n",
      "C:\\Users\\thinkpad\\AppData\\Local\\Temp\\ipykernel_3948\\2153889892.py:69: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['meas_prev'] = data.groupby('studentID')['meas'].shift(1)\n",
      "C:\\Users\\thinkpad\\AppData\\Local\\Temp\\ipykernel_3948\\2153889892.py:71: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['weeks_since_prev'] = (data['meas'] - data['meas_prev']).fillna(0).astype(int)\n",
      "C:\\Users\\thinkpad\\AppData\\Local\\Temp\\ipykernel_3948\\2153889892.py:73: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['weeks_since_first'] = (data['meas'] - data.groupby('studentID')['meas'].transform('first')).fillna(0).astype(int)\n",
      "C:\\Users\\thinkpad\\AppData\\Local\\Temp\\ipykernel_3948\\2153889892.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['obs_count'] = data.groupby('studentID').cumcount() + 1\n",
      "C:\\Users\\thinkpad\\AppData\\Local\\Temp\\ipykernel_3948\\2153889892.py:79: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  data = data.groupby('studentID').apply(lambda df: df.ffill()).reset_index(drop=True)\n",
      "C:\\Users\\thinkpad\\AppData\\Local\\Temp\\ipykernel_3948\\2153889892.py:79: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  data = data.groupby('studentID').apply(lambda df: df.ffill()).reset_index(drop=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 136\n",
      "Preprocessed data saved to: data/TX/tx_long_preprocessed.csv\n",
      "Resulting data shape: (2991, 142)\n",
      "\n",
      "Sample columns: ['Iv1_state', 'Av1_state', 'Uv1_state', 'Co1_state', 'Co2_state', 'Angst_abbruch_state', 'Angst_scheitern_state', 'Leist_verstehen_state', 'Leist_bearbeiten_state', 'Leist_stress_state', 'Leist_ueberfordert_state', 'Wiss_kommilitonen_state', 'Wiss_mathe_state', 'PANP01_state', 'PANP05_state', 'PANP08_state', 'PANN01_state', 'PANN05_state', 'PANN09_state', 'Iv1_state_lag1']\n",
      "\n",
      "Class distribution for next_event:\n",
      "next_event\n",
      "0    2948\n",
      "1      43\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Categorical features to pass to CatBoost: []\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('data/TX/tx_long_cleaned.csv')\n",
    "\n",
    "# Coerce to numeric; invalid entries become NaN and we'll drop them (or you can choose another strategy)\n",
    "data['meas'] = pd.to_numeric(data['meas'], errors='coerce')\n",
    "na_meas = int(data['meas'].isna().sum())\n",
    "if na_meas > 0:\n",
    "    print(f'Warning: {na_meas} rows have invalid/non-numeric meas and will be dropped')\n",
    "    data = data[~data['meas'].isna()].copy()\n",
    "\n",
    "# Convert to integer week numbers (if they are floats from CSV)\n",
    "data['meas'] = data['meas'].astype(int)\n",
    "\n",
    "# Sort by student and meas (week index)\n",
    "data = data.sort_values(['studentID', 'meas']).reset_index(drop=True)\n",
    "\n",
    "# 1) find first event time per student (earliest meas with event==1)\n",
    "first_evt = (\n",
    "    data[data['event'] == 1]\n",
    "        .groupby('studentID')['meas']\n",
    "        .min()\n",
    "        .rename('drpopout')\n",
    ")\n",
    "# merge first-event info back to data (NaN means never dropped)\n",
    "data = data.merge(first_evt, on='studentID', how='left')\n",
    "\n",
    "# 2) keep only at-risk rows: rows strictly before the first event (or all rows if student never dropped)\n",
    "atrisk = data[(data['drpopout'].isna()) | (data['meas'] < data['drpopout'])].copy()\n",
    "print(f\"Rows before filtering at-risk: {len(data)}, after at-risk filter: {len(atrisk)}\")\n",
    "\n",
    "# 3) build target: next-period dropout (t -> t+1)\n",
    "# compute on the full data so shift aligns with real next measurement; then filter keeps only at-risk rows\n",
    "data['next_event'] = data.groupby('studentID')['event'].shift(-1)\n",
    "\n",
    "# Now restrict to atrisk rows and drop rows where next_event is missing (last observation for a student)\n",
    "data = atrisk.merge(data[['studentID','meas','next_event']], on=['studentID','meas'], how='left')\n",
    "\n",
    "# Drop rows where we cannot build the target (last measurement per student)\n",
    "data = data[~data['next_event'].isna()].copy()\n",
    "data['next_event'] = data['next_event'].astype(int)\n",
    "\n",
    "# Identify numeric and categorical feature candidates (exclude identifiers and target)\n",
    "exclude_cols = {'studentID', 'meas', 'event', 'next_event', 'drpopout'}\n",
    "num_cols = [c for c in data.select_dtypes(include=[np.number]).columns if c not in exclude_cols]\n",
    "cat_cols = [c for c in data.select_dtypes(include=['object','category']).columns if c not in exclude_cols]\n",
    "\n",
    "print(f'Numeric features detected: {num_cols}')\n",
    "print(f'Categorical features detected: {cat_cols}')\n",
    "\n",
    "# Create lags and rolling features for numeric columns (use only past values)\n",
    "lags = [1,2,3]\n",
    "for lag in lags:\n",
    "    for c in num_cols:\n",
    "        data[f'{c}_lag{lag}'] = data.groupby('studentID')[c].shift(lag)\n",
    "\n",
    "# Rolling means using past values (shift by 1 to exclude current row)\n",
    "windows = [2,3,5]\n",
    "for w in windows:\n",
    "    for c in num_cols:\n",
    "        # shift(1) ensures only past values included\n",
    "        data[f'{c}_roll_mean_{w}'] = (\n",
    "            data.groupby('studentID')[c].shift(1)\n",
    "                .rolling(window=w, min_periods=1)\n",
    "                .mean()\n",
    "                .reset_index(level=0, drop=True)\n",
    "        )\n",
    "\n",
    "# Time-delta features using week-index (meas is integer week number)\n",
    "# previous meas (week index)\n",
    "data['meas_prev'] = data.groupby('studentID')['meas'].shift(1)\n",
    "# weeks since previous measurement (integer weeks). first observation will be NaN -> fill with 0\n",
    "data['weeks_since_prev'] = (data['meas'] - data['meas_prev']).fillna(0).astype(int)\n",
    "# weeks since first measurement\n",
    "data['weeks_since_first'] = (data['meas'] - data.groupby('studentID')['meas'].transform('first')).fillna(0).astype(int)\n",
    "# observation count\n",
    "data['obs_count'] = data.groupby('studentID').cumcount() + 1\n",
    "\n",
    "# Forward-fill within each student to propagate most recent past info\n",
    "# Note: forward-fill will not fill backward in time; since features are constructed from past, this is OK\n",
    "data = data.groupby('studentID').apply(lambda df: df.ffill()).reset_index(drop=True)\n",
    "\n",
    "# Impute remaining numerical NAs with median\n",
    "for c in data.select_dtypes(include=[np.number]).columns:\n",
    "    if c in exclude_cols:\n",
    "        continue\n",
    "    if data[c].isna().any():\n",
    "        med = data[c].median()\n",
    "        data[c] = data[c].fillna(med)\n",
    "\n",
    "# For categorical columns, fill NA with a placeholder and convert to category\n",
    "for c in cat_cols:\n",
    "    data[c] = data[c].fillna('missing').astype('category')\n",
    "\n",
    "# Final feature list (exclude ids and raw target)\n",
    "feature_cols = [c for c in data.columns if c not in ['studentID', 'meas', 'event', 'next_event', 'meas_prev', 'drpopout']]\n",
    "print('Number of features:', len(feature_cols))\n",
    "\n",
    "# Save preprocessed data\n",
    "out_path = 'data/TX/tx_long_preprocessed.csv'\n",
    "data.to_csv(out_path, index=False)\n",
    "print(f'Preprocessed data saved to: {out_path}')\n",
    "print('Resulting data shape:', data.shape)\n",
    "\n",
    "# Show a quick preview of top features and class balance\n",
    "print('\\nSample columns:', feature_cols[:20])\n",
    "print('\\nClass distribution for next_event:')\n",
    "print(data['next_event'].value_counts(normalize=False))\n",
    "\n",
    "# Reminder: when training CatBoost, pass categorical feature names in `cat_features` (cat_cols)\n",
    "print('\\nCategorical features to pass to CatBoost:', cat_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9daf8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## load data/T0/aist.csv\n",
    "bg = pd.read_csv('data/T0/background.csv')\n",
    "bg['age'] = 2025 - bg['gebu']   \n",
    "bg['age_centered'] = bg['age'] - bg['age'].mean()\n",
    "## delete gebu column\n",
    "bg = bg.drop(columns=['gebu'])\n",
    "# save processed background data\n",
    "bg.to_csv('data/T0/background_processed.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "42a22702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    studentID  bfi_ex  bfi_ve  bfi_ge  bfi_ne  bfi_of\n",
      "0    student1    2.67    2.33    4.33    4.33    4.00\n",
      "1   student10    2.33    2.33    3.33    2.67    3.67\n",
      "2  student100    2.00    3.33    2.33    1.33    4.00\n",
      "3  student101    1.67    2.67    2.67    3.33    4.67\n",
      "4  student102    3.67    3.33    3.67    2.33    4.00\n"
     ]
    }
   ],
   "source": [
    "big_five = pd.read_csv('data/T0/bfi.csv')\n",
    "print(big_five.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f893dcd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## merge all processed dataframes on studentID\n",
    "bg = pd.read_csv('data/T0/background_processed.csv')\n",
    "big_five = pd.read_csv('data/T0/bfi.csv')\n",
    "iq_data = pd.read_csv('data/T0/iq_processed.csv')\n",
    "math_tests = pd.read_csv('data/T0/math_test_processed.csv')\n",
    "#concat all dataframes on studentID\n",
    "merged = bg.merge(big_five, on='studentID', how='inner')\n",
    "merged = merged.merge(iq_data, on='studentID', how='inner')\n",
    "merged = merged.merge(math_tests, on='studentID', how='inner')\n",
    "## save merged data\n",
    "merged.to_csv('data/T0/merged_T0.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951d6425",
   "metadata": {},
   "outputs": [],
   "source": [
    "## merge T0 data into TX long data\n",
    "tx_long = pd.read_csv('data/TX/tx_long_preprocessed.csv')\n",
    "t0_t1 = tx_long.merge(merged, on='studentID', how='left')\n",
    "t0_t1.to_csv('data/TX/tx_long_preprocessed_with_T0.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205b4368",
   "metadata": {},
   "source": [
    "## Training datasets for CatBoost dropout model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ea23c9",
   "metadata": {},
   "source": [
    "### Training dataset for T1 only model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "72140adc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean AUC: 0.780 ± 0.054\n",
      "F1 Score: 0.185 ± 0.053\n",
      "Accuracy: 0.971 ± 0.013\n",
      "PR-AUC: 0.102 ± 0.055\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, f1_score, precision_recall_curve, average_precision_score\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "data = pd.read_csv('data/TX/tx_long_preprocessed.csv')\n",
    "X = data[feature_cols]\n",
    "y = data['next_event']\n",
    "weights = compute_class_weight('balanced', classes=np.array([0,1]), y=y)\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "auc_scores = []\n",
    "f1_scores = []\n",
    "accuracy_scores = []\n",
    "pr_auc_scores = []\n",
    "for train_idx, val_idx in kf.split(X, y):\n",
    "    train_pool = Pool(X.iloc[train_idx], y.iloc[train_idx])\n",
    "    val_pool   = Pool(X.iloc[val_idx], y.iloc[val_idx])\n",
    "\n",
    "    model = CatBoostClassifier(\n",
    "        depth=6, learning_rate=0.05, iterations=500,\n",
    "        l2_leaf_reg=5, class_weights=weights, random_strength=1.0, verbose=0\n",
    "    )\n",
    "    model.fit(train_pool, eval_set=val_pool, use_best_model=True)\n",
    "    y_prob = model.predict_proba(val_pool)[:,1]\n",
    "    y_valid = y.iloc[val_idx]\n",
    "    prec, rec, thr = precision_recall_curve(y_valid, y_prob)\n",
    "    f1 = 2 * prec * rec / (prec + rec + 1e-9)\n",
    "    best_t = thr[np.argmax(f1)]\n",
    "\n",
    "    y_pred = (y_prob >= best_t).astype(int)\n",
    "    auc = roc_auc_score(y_valid, y_prob)\n",
    "    f1_val = f1_score(y_valid, y_pred)\n",
    "    accuracy = accuracy_score(y_valid, y_pred)\n",
    "    pr_auc = average_precision_score(y_valid, y_prob)\n",
    "    auc_scores.append(auc)\n",
    "    f1_scores.append(f1_val)\n",
    "    accuracy_scores.append(accuracy)\n",
    "    pr_auc_scores.append(pr_auc)\n",
    "    \n",
    "\n",
    "print(f\"Mean AUC: {np.mean(auc_scores):.3f} ± {np.std(auc_scores):.3f}\")\n",
    "print(f\"F1 Score: {np.mean(f1_scores):.3f} ± {np.std(f1_scores):.3f}\")\n",
    "print(f\"Accuracy: {np.mean(accuracy_scores):.3f} ± {np.std(accuracy_scores):.3f}\")\n",
    "print(f\"PR-AUC: {np.mean(pr_auc_scores):.3f} ± {np.std(pr_auc_scores):.3f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ba14c9",
   "metadata": {},
   "source": [
    "### Training dataset for T1 + T0 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "96c0a128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean AUC: 0.759 ± 0.065\n",
      "F1 Score: 0.300 ± 0.046\n",
      "Accuracy: 0.973 ± 0.020\n",
      "PR-AUC: 0.186 ± 0.039\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, f1_score, precision_recall_curve, average_precision_score\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "\n",
    "data = pd.read_csv('data/TX/tx_long_preprocessed_with_t0.csv')\n",
    "feature_cols = [c for c in data.columns if c not in ['studentID', 'meas', 'event', 'next_event', 'meas_prev', 'drpopout']]\n",
    "X = data[feature_cols]\n",
    "y = data['next_event']\n",
    "weights = compute_class_weight('balanced', classes=np.array([0,1]), y=y)\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "auc_scores = []\n",
    "f1_scores = []\n",
    "accuracy_scores = []\n",
    "pr_auc_scores = []\n",
    "for train_idx, val_idx in kf.split(X, y):\n",
    "    train_pool = Pool(X.iloc[train_idx], y.iloc[train_idx])\n",
    "    val_pool   = Pool(X.iloc[val_idx], y.iloc[val_idx])\n",
    "\n",
    "    model = CatBoostClassifier(\n",
    "        depth=5, learning_rate=0.05, iterations=500,\n",
    "        l2_leaf_reg=8, class_weights=weights, random_strength=1.0, verbose=0\n",
    "    )\n",
    "    model.fit(train_pool, eval_set=val_pool, use_best_model=True)\n",
    "    y_prob = model.predict_proba(val_pool)[:,1]\n",
    "    y_valid = y.iloc[val_idx]\n",
    "    prec, rec, thr = precision_recall_curve(y_valid, y_prob)\n",
    "    f1 = 2 * prec * rec / (prec + rec + 1e-9)\n",
    "    best_t = thr[np.argmax(f1)]\n",
    "\n",
    "    y_pred = (y_prob >= best_t).astype(int)\n",
    "    auc = roc_auc_score(y_valid, y_prob)\n",
    "    f1_val = f1_score(y_valid, y_pred)\n",
    "    accuracy = accuracy_score(y_valid, y_pred)\n",
    "    pr_auc = average_precision_score(y_valid, y_prob)\n",
    "    auc_scores.append(auc)\n",
    "    f1_scores.append(f1_val)\n",
    "    accuracy_scores.append(accuracy)\n",
    "    pr_auc_scores.append(pr_auc)\n",
    "    \n",
    "\n",
    "print(f\"Mean AUC: {np.mean(auc_scores):.3f} ± {np.std(auc_scores):.3f}\")\n",
    "print(f\"F1 Score: {np.mean(f1_scores):.3f} ± {np.std(f1_scores):.3f}\")\n",
    "print(f\"Accuracy: {np.mean(accuracy_scores):.3f} ± {np.std(accuracy_scores):.3f}\")\n",
    "print(f\"PR-AUC: {np.mean(pr_auc_scores):.3f} ± {np.std(pr_auc_scores):.3f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22995ef3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Feature Id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Importances",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "b50de953-7a74-44d2-9528-4cead32ca0f4",
       "rows": [
        [
         "0",
         "weeks_since_first",
         "21.941373671100173"
        ],
        [
         "1",
         "math_test_centered",
         "11.020499573768337"
        ],
        [
         "2",
         "weeks_since_prev",
         "9.863661646889591"
        ],
        [
         "3",
         "age",
         "5.8417807266919946"
        ],
        [
         "4",
         "Wiss_kommilitonen_state",
         "5.745032532662309"
        ],
        [
         "5",
         "Wiss_kommilitonen_state_roll_mean_5",
         "5.647301143878578"
        ],
        [
         "6",
         "Angst_scheitern_state_lag1",
         "5.294276409996463"
        ],
        [
         "7",
         "Leist_ueberfordert_state_lag1",
         "4.514367494641607"
        ],
        [
         "8",
         "PANP08_state_roll_mean_3",
         "4.211694549659901"
        ],
        [
         "9",
         "Co1_state",
         "2.744936702832775"
        ],
        [
         "10",
         "Leist_verstehen_state_roll_mean_2",
         "2.5735656528877016"
        ],
        [
         "11",
         "Leist_stress_state_roll_mean_2",
         "2.492297692330967"
        ],
        [
         "12",
         "Wiss_mathe_state_lag2",
         "2.3011218998058944"
        ],
        [
         "13",
         "PANN05_state_lag1",
         "2.2632782447447637"
        ],
        [
         "14",
         "PANN09_state_lag3",
         "2.239265113888984"
        ],
        [
         "15",
         "PANP01_state_lag1",
         "1.9283468133548762"
        ],
        [
         "16",
         "Co2_state_roll_mean_5",
         "1.8833985047180928"
        ],
        [
         "17",
         "Wiss_kommilitonen_state_roll_mean_2",
         "1.8412766481020855"
        ],
        [
         "18",
         "Leist_ueberfordert_state_roll_mean_5",
         "1.7559051805136552"
        ],
        [
         "19",
         "Leist_ueberfordert_state",
         "1.7543036487071708"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 20
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature Id</th>\n",
       "      <th>Importances</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>weeks_since_first</td>\n",
       "      <td>21.941374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>math_test_centered</td>\n",
       "      <td>11.020500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>weeks_since_prev</td>\n",
       "      <td>9.863662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>age</td>\n",
       "      <td>5.841781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Wiss_kommilitonen_state</td>\n",
       "      <td>5.745033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Wiss_kommilitonen_state_roll_mean_5</td>\n",
       "      <td>5.647301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Angst_scheitern_state_lag1</td>\n",
       "      <td>5.294276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Leist_ueberfordert_state_lag1</td>\n",
       "      <td>4.514367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>PANP08_state_roll_mean_3</td>\n",
       "      <td>4.211695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Co1_state</td>\n",
       "      <td>2.744937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Leist_verstehen_state_roll_mean_2</td>\n",
       "      <td>2.573566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Leist_stress_state_roll_mean_2</td>\n",
       "      <td>2.492298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Wiss_mathe_state_lag2</td>\n",
       "      <td>2.301122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>PANN05_state_lag1</td>\n",
       "      <td>2.263278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>PANN09_state_lag3</td>\n",
       "      <td>2.239265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>PANP01_state_lag1</td>\n",
       "      <td>1.928347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Co2_state_roll_mean_5</td>\n",
       "      <td>1.883399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Wiss_kommilitonen_state_roll_mean_2</td>\n",
       "      <td>1.841277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Leist_ueberfordert_state_roll_mean_5</td>\n",
       "      <td>1.755905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Leist_ueberfordert_state</td>\n",
       "      <td>1.754304</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              Feature Id  Importances\n",
       "0                      weeks_since_first    21.941374\n",
       "1                     math_test_centered    11.020500\n",
       "2                       weeks_since_prev     9.863662\n",
       "3                                    age     5.841781\n",
       "4                Wiss_kommilitonen_state     5.745033\n",
       "5    Wiss_kommilitonen_state_roll_mean_5     5.647301\n",
       "6             Angst_scheitern_state_lag1     5.294276\n",
       "7          Leist_ueberfordert_state_lag1     4.514367\n",
       "8               PANP08_state_roll_mean_3     4.211695\n",
       "9                              Co1_state     2.744937\n",
       "10     Leist_verstehen_state_roll_mean_2     2.573566\n",
       "11        Leist_stress_state_roll_mean_2     2.492298\n",
       "12                 Wiss_mathe_state_lag2     2.301122\n",
       "13                     PANN05_state_lag1     2.263278\n",
       "14                     PANN09_state_lag3     2.239265\n",
       "15                     PANP01_state_lag1     1.928347\n",
       "16                 Co2_state_roll_mean_5     1.883399\n",
       "17   Wiss_kommilitonen_state_roll_mean_2     1.841277\n",
       "18  Leist_ueberfordert_state_roll_mean_5     1.755905\n",
       "19              Leist_ueberfordert_state     1.754304"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_feature_importance(prettified=True).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983fdc97",
   "metadata": {},
   "source": [
    "## Scales and Ranges for survey features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6d8c8890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sheet: aist.csv, Columns: ['Table name in frontend', 'Allgemeiner Interessen-Struktur-Test', 'Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4', 'Unnamed: 5']\n",
      "Sheet: background.csv, Columns: ['Table name in frontend', 'Profil / Hintergrundinformationen', 'Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4']\n",
      "Sheet: bfi.csv, Columns: ['Table name in frontend', 'Persönlichkeitsskala (Big-Five)', 'Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4']\n",
      "Sheet: iq.csv, Columns: ['Table name in frontend', 'Kognitive Fähigkeiten', 'Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4']\n",
      "Sheet: kont.csv, Columns: ['Table name in frontend', 'Internale-Externale-Kontrollüberzeugung', 'Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4']\n",
      "Sheet: math_test.csv, Columns: ['Table name in frontend', 'Fachwissenstest Mathematik', 'Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4']\n",
      "Sheet: motivation.csv, Columns: ['Table name in frontend', 'Motivation', 'Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4']\n",
      "Sheet: panas.csv, Columns: ['Table name in frontend', 'PANAS (Positive and Negative Affect Schedule)', 'Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4']\n",
      "Sheet: tx_long.csv, Columns: ['Table name in frontend', '-', 'Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4', 'Unnamed: 5']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "## upload data/TX/data_scales.xlsx\n",
    "scales = pd.read_excel('data/TX/data_scales.xlsx', sheet_name=None)\n",
    "## columns of each sheet\n",
    "for sheet_name, df in scales.items():\n",
    "    print(f'Sheet: {sheet_name}, Columns: {df.columns.tolist()}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d991e4",
   "metadata": {},
   "source": [
    "### TX scales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "042e3bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tx_scale = scales['tx_long.csv']\n",
    "tx_scale = tx_scale.iloc[4:].reset_index(drop=True)\n",
    "## delete second column\n",
    "tx_scale = tx_scale.drop(columns=[tx_scale.columns[1]])\n",
    "# print(tx_scale.head())\n",
    "## rename column names as Columns, scale, range, general_name, prompt_in_questionnaire\n",
    "tx_scale.columns = ['Columns', 'scale', 'range', 'general_name', 'prompt_in_questionnaire']\n",
    "# delete the rows where Columns is NaN\n",
    "tx_scale = tx_scale[~tx_scale['Columns'].isna()]\n",
    "## save tx_scale to csv\n",
    "tx_scale.to_csv('data/TX/tx_scale.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61018b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "TX_scales = pd.read_csv('data/TX/tx_scale.csv')\n",
    "TX_features = [c for c in TX_scales['Columns'] if c not in ['studentID', 'meas', 'event', 'Summe']]\n",
    "prompt_TX = {\n",
    "    row['Columns']: row['prompt_in_questionnaire'] \n",
    "    for _, row in TX_scales.iterrows() \n",
    "    if row['Columns'] in TX_features\n",
    "}\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa7823c",
   "metadata": {},
   "source": [
    "### T0 Scales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24b56e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Columns",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "scale",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "range",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "general_name",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "bc502933-9c66-4a16-b66c-da5a0f1e7346",
       "rows": [
        [
         "0",
         "bfi_ex",
         "float",
         "1 to 5",
         "Extraversion"
        ],
        [
         "1",
         "bfi_ve",
         "float",
         "1 to 5",
         "Verträglichkeit"
        ],
        [
         "2",
         "bfi_ge",
         "float",
         "1 to 5",
         "Gewissenhaftigkeit"
        ],
        [
         "3",
         "bfi_ne",
         "float",
         "1 to 5",
         "Negative Emotionalität"
        ],
        [
         "4",
         "bfi_of",
         "float",
         "1 to 5",
         "Offenheit"
        ]
       ],
       "shape": {
        "columns": 4,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Columns</th>\n",
       "      <th>scale</th>\n",
       "      <th>range</th>\n",
       "      <th>general_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bfi_ex</td>\n",
       "      <td>float</td>\n",
       "      <td>1 to 5</td>\n",
       "      <td>Extraversion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bfi_ve</td>\n",
       "      <td>float</td>\n",
       "      <td>1 to 5</td>\n",
       "      <td>Verträglichkeit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bfi_ge</td>\n",
       "      <td>float</td>\n",
       "      <td>1 to 5</td>\n",
       "      <td>Gewissenhaftigkeit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bfi_ne</td>\n",
       "      <td>float</td>\n",
       "      <td>1 to 5</td>\n",
       "      <td>Negative Emotionalität</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bfi_of</td>\n",
       "      <td>float</td>\n",
       "      <td>1 to 5</td>\n",
       "      <td>Offenheit</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Columns  scale   range            general_name\n",
       "0  bfi_ex  float  1 to 5            Extraversion\n",
       "1  bfi_ve  float  1 to 5         Verträglichkeit\n",
       "2  bfi_ge  float  1 to 5      Gewissenhaftigkeit\n",
       "3  bfi_ne  float  1 to 5  Negative Emotionalität\n",
       "4  bfi_of  float  1 to 5               Offenheit"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# delete first 5 rows and second column\n",
    "bf1_scales = bf1_scales.iloc[5:].reset_index(drop=True)\n",
    "bf1_scales = bf1_scales.drop(columns=[bf1_scales.columns[1]])\n",
    "bf1_scales.columns = ['Columns', 'scale', 'range', 'general_name']\n",
    "bf1_scales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d32aa8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Table name in frontend",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "Persönlichkeitsskala (Big-Five)",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Unnamed: 2",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "Unnamed: 3",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "Unnamed: 4",
         "rawType": "object",
         "type": "unknown"
        }
       ],
       "ref": "ceac5067-37e2-46fe-807f-2a5374fbd6fd",
       "rows": [
        [
         "0",
         null,
         null,
         null,
         null,
         null
        ],
        [
         "1",
         null,
         null,
         null,
         null,
         null
        ],
        [
         "2",
         null,
         null,
         null,
         null,
         null
        ],
        [
         "3",
         "Columns",
         null,
         "scale",
         "range",
         "general name"
        ],
        [
         "4",
         "studentID",
         null,
         "string",
         "student1:N",
         "ID"
        ],
        [
         "5",
         "bfi_ex",
         null,
         "float",
         "1 to 5",
         "Extraversion"
        ],
        [
         "6",
         "bfi_ve",
         null,
         "float",
         "1 to 5",
         "Verträglichkeit"
        ],
        [
         "7",
         "bfi_ge",
         null,
         "float",
         "1 to 5",
         "Gewissenhaftigkeit"
        ],
        [
         "8",
         "bfi_ne",
         null,
         "float",
         "1 to 5",
         "Negative Emotionalität"
        ],
        [
         "9",
         "bfi_of",
         null,
         "float",
         "1 to 5",
         "Offenheit"
        ]
       ],
       "shape": {
        "columns": 5,
        "rows": 10
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Table name in frontend</th>\n",
       "      <th>Persönlichkeitsskala (Big-Five)</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Columns</td>\n",
       "      <td>NaN</td>\n",
       "      <td>scale</td>\n",
       "      <td>range</td>\n",
       "      <td>general name</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>studentID</td>\n",
       "      <td>NaN</td>\n",
       "      <td>string</td>\n",
       "      <td>student1:N</td>\n",
       "      <td>ID</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>bfi_ex</td>\n",
       "      <td>NaN</td>\n",
       "      <td>float</td>\n",
       "      <td>1 to 5</td>\n",
       "      <td>Extraversion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>bfi_ve</td>\n",
       "      <td>NaN</td>\n",
       "      <td>float</td>\n",
       "      <td>1 to 5</td>\n",
       "      <td>Verträglichkeit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>bfi_ge</td>\n",
       "      <td>NaN</td>\n",
       "      <td>float</td>\n",
       "      <td>1 to 5</td>\n",
       "      <td>Gewissenhaftigkeit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>bfi_ne</td>\n",
       "      <td>NaN</td>\n",
       "      <td>float</td>\n",
       "      <td>1 to 5</td>\n",
       "      <td>Negative Emotionalität</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>bfi_of</td>\n",
       "      <td>NaN</td>\n",
       "      <td>float</td>\n",
       "      <td>1 to 5</td>\n",
       "      <td>Offenheit</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Table name in frontend  Persönlichkeitsskala (Big-Five) Unnamed: 2  \\\n",
       "0                    NaN                              NaN        NaN   \n",
       "1                    NaN                              NaN        NaN   \n",
       "2                    NaN                              NaN        NaN   \n",
       "3                Columns                              NaN      scale   \n",
       "4              studentID                              NaN     string   \n",
       "5                 bfi_ex                              NaN      float   \n",
       "6                 bfi_ve                              NaN      float   \n",
       "7                 bfi_ge                              NaN      float   \n",
       "8                 bfi_ne                              NaN      float   \n",
       "9                 bfi_of                              NaN      float   \n",
       "\n",
       "   Unnamed: 3              Unnamed: 4  \n",
       "0         NaN                     NaN  \n",
       "1         NaN                     NaN  \n",
       "2         NaN                     NaN  \n",
       "3       range            general name  \n",
       "4  student1:N                      ID  \n",
       "5      1 to 5            Extraversion  \n",
       "6      1 to 5         Verträglichkeit  \n",
       "7      1 to 5      Gewissenhaftigkeit  \n",
       "8      1 to 5  Negative Emotionalität  \n",
       "9      1 to 5               Offenheit  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bf1_scales = scales['bfi.csv']\n",
    "bg_scales = scales['background.csv']\n",
    "iq_scales = scales['iq.csv']\n",
    "math_test = scales['math_test.csv']\n",
    "for sc_df, name in zip(\n",
    "    [bf1_scales, bg_scales, iq_scales, math_test],\n",
    "    ['bfi', 'background', 'iq', 'math_test']\n",
    "):\n",
    "    sc_df = sc_df.iloc[5:].reset_index(drop=True)\n",
    "    sc_df = sc_df.drop(columns=[sc_df.columns[1]])\n",
    "    sc_df.columns = ['Columns', 'scale', 'range', 'general_name']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce7ba1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bf1 = pd.read_csv('data/T0/bfi_scale.csv')\n",
    "bg = pd.read_csv('data/T0/background_scale.csv')\n",
    "iq = pd.read_csv('data/T0/iq_scale.csv')\n",
    "math = pd.read_csv('data/T0/math_test_scale.csv')\n",
    "## merge all scales into a single dataframe\n",
    "T0_scales = pd.concat([bg, bf1, iq, math], ignore_index=True)\n",
    "# change the last variable in T0_scales['Columns'] =='fw_pkt' to 'math_total_score'\n",
    "T0_scales.loc[T0_scales['Columns'] == 'fw_pkt', 'Columns'] = 'math_total_score'\n",
    "## save all scales to csv\n",
    "T0_scales.to_csv('data/T0/T0_scales.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
